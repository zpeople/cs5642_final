{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/competitions/feedback-prize-effectiveness/overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PD Setting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 显示所有列\n",
    "pd.set_option('display.max_columns', None)\n",
    "# 显示所有行\n",
    "pd.set_option('display.max_rows', None)\n",
    "#显示宽度\n",
    "pd.set_option('display.width', 2000)\n",
    "pd.set_option('display.max_colwidth',2000 )\n",
    "pd.set_option('display.unicode.ambiguous_as_wide', True)\n",
    "pd.set_option('display.unicode.east_asian_width', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "实验数据大小: (36765, 5)\n",
      "预测数据大小: (10, 4)\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "train_path =\"./data/train.csv\"\n",
    "test_path=\"./data/test.csv\"\n",
    "train_data = pd.read_csv(train_path)\n",
    "test_data =pd.read_csv(test_path)\n",
    "print('实验数据大小:',train_data.shape)\n",
    "print('预测数据大小:',test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* discourse_id - ID code for discourse element \n",
    "* essay_id - ID code for essay response. This ID code corresponds to the name of the full-text file in the train/ folder.\n",
    "* discourse_text - Text of discourse element. \n",
    "* discourse_type - Class label of discourse element. \n",
    "* discourse_type_num - Enumerated class label of discourse element . \n",
    "* discourse_effectiveness - Quality rating of discourse element, the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 36765 entries, 0 to 36764\n",
      "Data columns (total 5 columns):\n",
      " #   Column                   Non-Null Count  Dtype \n",
      "---  ------                   --------------  ----- \n",
      " 0   discourse_id             36765 non-null  object\n",
      " 1   essay_id                 36765 non-null  object\n",
      " 2   discourse_text           36765 non-null  object\n",
      " 3   discourse_type           36765 non-null  object\n",
      " 4   discourse_effectiveness  36765 non-null  object\n",
      "dtypes: object(5)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>essay_id</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_effectiveness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>36765</td>\n",
       "      <td>36765</td>\n",
       "      <td>36765</td>\n",
       "      <td>36765</td>\n",
       "      <td>36765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>36765</td>\n",
       "      <td>4191</td>\n",
       "      <td>36691</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>0013cc385424</td>\n",
       "      <td>91B1F82B2CF1</td>\n",
       "      <td>Summer projects should be student-designed</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Adequate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "      <td>12105</td>\n",
       "      <td>20977</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        discourse_id      essay_id                               discourse_text discourse_type discourse_effectiveness\n",
       "count          36765         36765                                        36765          36765                   36765\n",
       "unique         36765          4191                                        36691              7                       3\n",
       "top     0013cc385424  91B1F82B2CF1  Summer projects should be student-designed        Evidence                Adequate\n",
       "freq               1            23                                           14          12105                   20977"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "display 查看所有columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Lead - an introduction that begins with a statistic, a quotation, a description, or some other device to grab the reader’s attention and point toward the thesis\n",
    "* Position - an opinion or conclusion on the main question\n",
    "* Claim - a claim that supports the position\n",
    "* Counterclaim - a claim that refutes another claim or gives an opposing reason to the position\n",
    "* Rebuttal - a claim that refutes a counterclaim\n",
    "* Evidence - ideas or examples that support claims, counterclaims, or rebuttals.\n",
    "* Concluding Statement - a concluding statement that restates the claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "discourse_type\n",
       "Evidence                0.329253\n",
       "Claim                   0.325772\n",
       "Position                0.109452\n",
       "Concluding Statement    0.091146\n",
       "Lead                    0.062315\n",
       "Counterclaim            0.048225\n",
       "Rebuttal                0.033837\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['discourse_type'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our task is to predict the quality rating of each discourse element. Human readers rated each rhetorical or argumentative element, in order of increasing quality, as one of :\n",
    "\n",
    "* Ineffective\n",
    "* Adequate\n",
    "* Effective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "discourse_effectiveness\n",
       "Adequate       0.570570\n",
       "Effective      0.253665\n",
       "Ineffective    0.175765\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['discourse_effectiveness'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "head()\n",
    "\n",
    "用于显示 DataFrame 的前几行数据的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>essay_id</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_effectiveness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0013cc385424</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>Hi, i'm Isaac, i'm going to be writing about how this face on Mars is a natural landform or if there is life on Mars that made it. The story is about how NASA took a picture of Mars and a face was seen on the planet. NASA doesn't know if the landform was created by life on Mars, or if it is just a natural landform.</td>\n",
       "      <td>Lead</td>\n",
       "      <td>Adequate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9704a709b505</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>On my perspective, I think that the face is a natural landform because I dont think that there is any life on Mars. In these next few paragraphs, I'll be talking about how I think that is is a natural landform</td>\n",
       "      <td>Position</td>\n",
       "      <td>Adequate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c22adee811b6</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>I think that the face is a natural landform because there is no life on Mars that we have descovered yet</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Adequate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a10d361e54e4</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>If life was on Mars, we would know by now. The reason why I think it is a natural landform because, nobody live on Mars in order to create the figure. It says in paragraph 9, \"It's not easy to target Cydonia,\" in which he is saying that its not easy to know if it is a natural landform at this point. In all that they're saying, its probably a natural landform.</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Adequate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>db3e453ec4e2</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>People thought that the face was formed by alieans because they thought that there was life on Mars.</td>\n",
       "      <td>Counterclaim</td>\n",
       "      <td>Adequate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   discourse_id      essay_id                                                                                                                                                                                                                                                                                                                                                              discourse_text discourse_type discourse_effectiveness\n",
       "0  0013cc385424  007ACE74B050                                               Hi, i'm Isaac, i'm going to be writing about how this face on Mars is a natural landform or if there is life on Mars that made it. The story is about how NASA took a picture of Mars and a face was seen on the planet. NASA doesn't know if the landform was created by life on Mars, or if it is just a natural landform.            Lead                Adequate\n",
       "1  9704a709b505  007ACE74B050                                                                                                                                                          On my perspective, I think that the face is a natural landform because I dont think that there is any life on Mars. In these next few paragraphs, I'll be talking about how I think that is is a natural landform        Position                Adequate\n",
       "2  c22adee811b6  007ACE74B050                                                                                                                                                                                                                                                                   I think that the face is a natural landform because there is no life on Mars that we have descovered yet           Claim                Adequate\n",
       "3  a10d361e54e4  007ACE74B050  If life was on Mars, we would know by now. The reason why I think it is a natural landform because, nobody live on Mars in order to create the figure. It says in paragraph 9, \"It's not easy to target Cydonia,\" in which he is saying that its not easy to know if it is a natural landform at this point. In all that they're saying, its probably a natural landform.        Evidence                Adequate\n",
       "4  db3e453ec4e2  007ACE74B050                                                                                                                                                                                                                                                                       People thought that the face was formed by alieans because they thought that there was life on Mars.    Counterclaim                Adequate"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "isnull().sum()\n",
    "\n",
    "用来表示数据中的缺失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_values = train_data.isna().sum()\n",
    "# 或者\n",
    "missing_values = train_data.isnull().sum()\n",
    "\n",
    "# 查看缺失值\n",
    "missing_values[missing_values > 0]\n",
    "\n",
    "\n",
    "# 查看某列包含缺失值（NaN）的前几条记录\n",
    "# train_data[train_data['column_name'].isnull()].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".nunique()\n",
    "\n",
    "用于返回对象中除 NA/null 值外的不同(unique)的非重复值的数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "discourse_id               36765\n",
       "essay_id                    4191\n",
       "discourse_text             36691\n",
       "discourse_type                 7\n",
       "discourse_effectiveness        3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".unique()\n",
    "\n",
    "查看某列里，所有不同的（唯一）值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".corr()\n",
    "\n",
    "* 'pearson'：标准的皮尔逊相关系数。\n",
    "* 'kendall'：Kendall Tau 相关系数。\n",
    "* 'spearman'：Spearman 秩相关系数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the proportion of missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the relationship between features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View Distribution of each type of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#正负数据平衡才有参考意义\n",
    "df_majority = train_data[train_data['target'] == 0]\n",
    "df_minority = train_data[train_data['target'] == 1]\n",
    "df_majority_downsampled = df_majority.sample(n=len(df_minority), random_state=42)\n",
    "train_data = pd.concat([df_majority_downsampled, df_minority], axis=0)\n",
    "\n",
    "\n",
    "\n",
    "def ViewCategoricalData(data, cat_cols):\n",
    "    f = pd.melt(data, id_vars=['target'], value_vars=cat_cols)\n",
    "    g = sns.FacetGrid(f, col=\"variable\", hue=\"target\", col_wrap=3, sharex=False, sharey=False)\n",
    "    g = g.map(sns.countplot, \"value\", alpha=0.6).add_legend()\n",
    "\n",
    "    for ax in g.axes.flat:\n",
    "        labels = ax.get_xticklabels()\n",
    "        ax.set_xticks(range(len(labels)))\n",
    "        ax.set_xticklabels(labels, rotation=45)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def ViewContinuousData(data, num_cols):\n",
    "    f = pd.melt(data, id_vars=['target'], value_vars=num_cols)\n",
    "    g = sns.FacetGrid(f, col=\"variable\", hue=\"target\", col_wrap=3, sharex=False, sharey=False)\n",
    "    g = g.map(sns.kdeplot, \"value\", alpha=0.6).add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "* 数据清洗（Data Cleaning）：\n",
    "  \n",
    "处理缺失值：可以通过删除含有缺失值的记录、填充缺失值（如使用均值、中位数或众数填充）等方式进行。\n",
    "\n",
    "去除重复值：识别并删除数据集中的重复记录。\n",
    "\n",
    "纠正错误值：检查数据中的逻辑错误或异常值，并进行相应的修正。\n",
    "\n",
    "* 数据转换（Data Transformation）：\n",
    "  \n",
    "标准化/归一化：将数值属性缩放到某个特定范围，比如0-1之间，或者将其标准化以拥有零均值和单位方差。\n",
    "\n",
    "编码分类变量：如你之前提到的，可以使用LabelEncoder或OneHotEncoder对分类数据进行编码。\n",
    "\n",
    "特征构造：基于现有特征创建新的有意义的特征。\n",
    "\n",
    "* 数据集成（Data Integration）：\n",
    "  \n",
    "合并来自不同源的数据，确保一致性，解决实体识别问题，以及处理冗余和冲突的数据。\n",
    "\n",
    "* 数据规约（Data Reduction）：\n",
    "  \n",
    "降维：通过主成分分析（PCA）、线性判别分析（LDA）等技术减少数据维度。\n",
    "\n",
    "数量规约：使用参数模型或非参数模型来代替原始数据。\n",
    "\n",
    "* 数据离散化（Data Discretization）：\n",
    "  \n",
    "将连续型数据转换为离散区间，便于某些算法处理。\n",
    "\n",
    "* 数据分割（Data Splitting）：\n",
    "  \n",
    "将数据集划分为训练集、验证集和测试集，用于模型训练、调参和评估。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 整合数据，可以一起处理\n",
    "full_data = pd.concat([train_data,test_data],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning\n",
    "\n",
    "#用一个特定的常数来填充所有的缺失值\n",
    "full_data.fillna(0, inplace=True) \n",
    "\n",
    "# 用特定字母\n",
    "full_data['column_name'].fillna('U')\n",
    "\n",
    "#使用上一个非缺失值来填充当前的缺失值。这种方法通常适用于时间序列数据\n",
    "full_data.fillna(method='ffill', inplace=True)\n",
    "\n",
    "#与前向填充相反，使用下一个非缺失值来填充当前的缺失值\n",
    "full_data.fillna(method='bfill', inplace=True)\n",
    "\n",
    "#均值/中位数/众数填充\n",
    "# 使用均值填充\n",
    "full_data['column_name'].fillna(full_data['column_name'].mean(), inplace=True)\n",
    "\n",
    "# 使用中位数填充\n",
    "full_data['column_name'].fillna(full_data['column_name'].median(), inplace=True)\n",
    "\n",
    "# 使用众数填充\n",
    "full_data['column_name'].fillna(full_data['column_name'].mode()[0], inplace=True)\n",
    "\n",
    "#插值法（Interpolation） 特别适合于时间序列数据，可以根据已有的数据点估计缺失值\n",
    "full_data.interpolate(method='linear', inplace=True)\n",
    "\n",
    "#出现频率最高的值来填充所有的缺失值\n",
    "full_data = full_data.fillna({'column_name': full_data['column_name'].value_counts().idxmax()})\n",
    "\n",
    "#从 Cabin 列中提取每个客舱的第一个字符来表示甲板（Deck）信息，并创建一个新的列 Deck 来存储这些信息。\n",
    "full_data['Deck'] = full_data['Cabin'].apply(lambda x: x[0] if x != 'Unknown' else 'Unknown')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Drop\n",
    "\n",
    "# 删除所有 column_name 列中值为 'some_value' 的行。\n",
    "full_data.drop(full_data[full_data['column_name'] == 'some_value'].index, inplace=True)\n",
    "\n",
    "# 删除含有任何缺失值的行\n",
    "full_data.dropna(inplace=True)\n",
    "\n",
    "# 删除含有任何缺失值的列\n",
    "full_data.dropna(axis=1, inplace=True)\n",
    "\n",
    "# 删除指定列\n",
    "full_data.drop('column_name', axis=1, inplace=True)\n",
    "full_data.drop(['column_name1', 'column_name2'], axis=1, inplace=True)\n",
    "\n",
    "#基于条件删除列\n",
    "thresh = len(full_data) * 0.6  # 至少需要60%的非空值\n",
    "full_data.dropna(axis=1, thresh=thresh, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Transformation\n",
    "#  Rank Gauss Transformation\n",
    "from scipy.special import erfinv\n",
    "\n",
    "def rank_gauss(x):\n",
    "    x = x.argsort().argsort() # rank\n",
    "    x = (x/x.max()-0.5)*2 #转换尺度到[-1,1]\n",
    "    epsilon=1e-6\n",
    "    x = np.clip(x, -1+epsilon, 1-epsilon)\n",
    "    x = erfinv(x) \n",
    "    return x\n",
    "\n",
    "for col in FLOAT_COLS:\n",
    "    if col in train_data.columns:\n",
    "        print(col,train_data[col].mean(), train_data[col].std())\n",
    "        train_data[col] = rank_gauss(train_data[col])\n",
    "\n",
    "for col in FLOAT_COLS:\n",
    "    if col in train_data.columns:\n",
    "        print(col,test_data[col].mean(), test_data[col].std())\n",
    "        test_data[col] = rank_gauss(test_data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Transformation   Min-Max 归一化\n",
    "# 将数据缩放到指定的范围（通常为 [0, 1] 或 [-1, 1]）。\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))  # 缩放到 [0, 1]\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(train_data), columns=train_data.columns)\n",
    "#在测试集上应用相同的参数。避免直接在测试集上拟合，以防止数据泄露\n",
    "test_data_scaled = scaler.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Transformation   Z-Score 标准化\n",
    "# 将数据缩放到指定的范围（通常为 [0, 1] 或 [-1, 1]）。\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df_standardized = pd.DataFrame(scaler.fit_transform(train_data), columns=train_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Transformation   MaxAbs 归一化\n",
    "# 将数据缩放到 [-1, 1] 范围，适合稀疏数据。\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "scaler = MaxAbsScaler()\n",
    "df_maxabs = pd.DataFrame(scaler.fit_transform(train_data), columns=train_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Transformation\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "# 避免污染到原始数据\n",
    "full_data =train_data.copy() \n",
    "# 对每个分类特征应用标签编码\n",
    "for column in full_data.columns:\n",
    "    if(full_data[column].dtype =='object'):\n",
    "        full_data[column] = le.fit_transform(full_data[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Discretization\n",
    "\n",
    "# 对数变换 有助于减少数据中的偏斜(skewness)，特别是当存在大量小值和少量极大值时 使得数据分布更加接近正态分布\n",
    "full_data['column_name'] = np.log1p(full_data['column_name'])\n",
    "\n",
    "# 找出数值型特征（列），并计算这些特征的偏度（skewness）。然后，它会筛选出那些偏度绝对值大于1的特征，并准备对这些特征进行对数变换以减少数据偏斜\n",
    "numeric_df = full_data.select_dtypes(['float64','int32','int64'])\n",
    "numeric_cols = numeric_df.columns.tolist()\n",
    "skewed_cols = full_data[numeric_cols].apply(lambda x: x.skew()).sort_values(ascending=False)\n",
    "skewed_df = pd.DataFrame({'skew':skewed_cols})\n",
    "skew_cols = skewed_df[skewed_df['skew'].abs()>1].index.tolist()\n",
    "for col in skew_cols:\n",
    "    # 避免对含有0或负数的列直接取对数，可以先加上一个最小正值\n",
    "    min_val = full_data[col].min()\n",
    "    if min_val <= 0:\n",
    "        full_data[col] = full_data[col] - min_val + 1e-8  # 调整值域使所有值都大于0\n",
    "    full_data[col] = np.log(full_data[col])\n",
    "\n",
    "\n",
    "# 分箱（Binning）是一种将连续数据离散化的方法，可以减少异常值的影响，使模型更加稳定  \n",
    "# 划分到5个等宽的区间（bins）\n",
    "full_data['column_name'] = pd.cut(full_data['column_name'], bins=5, labels=False)\n",
    "\n",
    "\n",
    "#按自己的需求划分区间\n",
    "full_data.loc[full_data['column_name'] <= 7.91, 'column_name'] = 0\n",
    "full_data.loc[(full_data['column_name'] > 7.91) & (full_data['column_name'] <= 14.454), 'column_name'] = 1\n",
    "full_data.loc[(full_data['column_name'] > 14.454) & (full_data['column_name'] <= 31), 'column_name'] = 2\n",
    "full_data.loc[full_data['column_name'] > 31, 'column_name'] = 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import pandas as pd\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "# retrieve english stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Converts text to lower case\n",
    "def convert_to_lowercase(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    if isinstance(text, str):\n",
    "        return text.lower()\n",
    "    return text\n",
    "\n",
    "# Remove all punctuation from the text\n",
    "def remove_punctuation(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    text = re.sub(f'[{string.punctuation}]', '', text)\n",
    "    return text\n",
    "\n",
    "# Removes all numbers from the text\n",
    "def remove_numbers(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    return text\n",
    "\n",
    "# Text segmentation, then remove the length of 2 or less and the single word and stop word\n",
    "def remove_short_words_and_stop_words(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    words = word_tokenize(text)\n",
    "    words = [word for word in words if len(word) > 2 and word not in stop_words]\n",
    "    cleaned_text = ' '.join(words)\n",
    "    return cleaned_text\n",
    "\n",
    "# Replace two or more consecutive Spaces with a single space\n",
    "def remove_multiple_spaces(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    cleaned_text = re.sub(r' {2,}', ' ', text)\n",
    "    return cleaned_text\n",
    "\n",
    "# Remove urls\n",
    "def remove_urls(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    url_pattern = r'(www.|http[s]?://)(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    return re.sub(url_pattern, '', text)\n",
    "\n",
    "# Remove hmtmls\n",
    "def remove_html(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    html_entities = r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});'\n",
    "    return re.sub(html_entities, '', text)\n",
    "\n",
    "# Remove @ and #   \n",
    "# 变成小写字母才有效 \n",
    "def remove_tags(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    tag_pattern = r'@([a-z0-9]+)|#'\n",
    "    return re.sub(tag_pattern, '', text)\n",
    "\n",
    "def remove_emoji(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z\\d\\s]+', '',text)\n",
    "    word_list = []\n",
    "    for each_word in cleaned_text.split(' '):\n",
    "        word_list.append((each_word).lower())\n",
    "    word_list = [\n",
    "        WordNetLemmatizer().lemmatize(each_word.strip()) for each_word in word_list\n",
    "        if each_word not in stop_words and each_word.strip() != ''\n",
    "    ]\n",
    "    return \" \".join(word_list)\n",
    "\n",
    "#将驼峰命名法（CamelCase）的字符串拆分为单词\n",
    "\n",
    "def split_camel_case(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    return re.sub(r'(?<!^)([A-Z])', r' \\1', text)\n",
    "\n",
    "# 形如 example.com 的域名替换为 example com  将域名中的点（.）替换为空格\n",
    "def preserve_domain(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    return re.sub(r'([a-zA-Z0-9]+)\\.([a-zA-Z]{2,})', r'\\1 \\2', text)\n",
    "\n",
    "def clean_text_bycol(df, col):\n",
    "    df[col] = df[col].apply(split_camel_case)\n",
    "    df[col] = df[col].apply(convert_to_lowercase)\n",
    "    df[col] = df[col].apply(remove_urls)\n",
    "    df[col] = df[col].apply(remove_html)\n",
    "    df[col] = df[col].apply(remove_tags)\n",
    "    df[col] = df[col].apply(remove_numbers)\n",
    "    df[col] = df[col].apply(remove_short_words_and_stop_words)\n",
    "    df[col] = df[col].apply(remove_emoji) \n",
    "    df[col] = df[col].apply(remove_multiple_spaces)\n",
    "    df[col] = df[col].apply(preserve_domain)\n",
    "    df[col] = df[col].apply(remove_punctuation) \n",
    "    return df\n",
    "\n",
    "def clean_text(text):\n",
    "    print(text)\n",
    "    text =split_camel_case(text)\n",
    "    text =convert_to_lowercase(text)\n",
    "    text =remove_urls(text)\n",
    "    text =remove_html(text)\n",
    "    text =remove_tags(text)\n",
    "    text =remove_numbers(text)\n",
    "    text =remove_short_words_and_stop_words(text)\n",
    "    text =remove_emoji(text)\n",
    "    text =remove_multiple_spaces(text)\n",
    "    text =preserve_domain(text)\n",
    "    text =remove_punctuation(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the evaluation metrics that we will use to assess our models\n",
    "\n",
    "def accuracy(predicted_labels, true_labels):\n",
    "    \"\"\"\n",
    "    Accuracy is correct predictions / all predicitons\n",
    "    \n",
    "    Args:\n",
    "        predicted_labels (np.ndarray[int, 1]): the integer labels from the predictions. Uni-dimensional\n",
    "        true_labels (np.ndarray[int, 1]): the integer labels from the gold standard. Uni-dimensional\n",
    "    \n",
    "    Returns:\n",
    "        accuracy_value (double)\n",
    "        \n",
    "    \"\"\"\n",
    "    accuracy_value = 0.\n",
    "    accuracy_value = predicted_labels[predicted_labels == true_labels].shape[0] / predicted_labels.shape[0]\n",
    "    return accuracy_value\n",
    "\n",
    "def precision(predicted_labels, true_labels):\n",
    "    \"\"\"\n",
    "    Precision is True Positives / All Positives Predictions\n",
    "    \n",
    "    Args:\n",
    "        predicted_labels (np.ndarray[int, 1]): the integer labels from the predictions. Uni-dimensional\n",
    "        true_labels (np.ndarray[int, 1]): the integer labels from the gold standard. Uni-dimensional\n",
    "    \n",
    "    Returns:\n",
    "        precision_value (double)\n",
    "        \n",
    "    \"\"\"\n",
    "    precision_value = 0.\n",
    "    TP = np.sum((predicted_labels == 1) & (true_labels == 1))\n",
    "    FP = np.sum((predicted_labels == 1) & (true_labels == 0))\n",
    "    precision_value = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    return precision_value\n",
    "\n",
    "def recall(predicted_labels, true_labels):\n",
    "    \"\"\"\n",
    "    Recall is True Positives / All Positive Labels\n",
    "    \n",
    "    Args:\n",
    "        predicted_labels (np.ndarray[int, 1]): the integer labels from the predictions. Uni-dimensional\n",
    "        true_labels (np.ndarray[int, 1]): the integer labels from the gold standard. Uni-dimensional\n",
    "    \n",
    "    Returns:\n",
    "        recall_value (double)\n",
    "        \n",
    "    \"\"\"\n",
    "    recall_value = 0.\n",
    "    TP = np.sum((predicted_labels == 1) & (true_labels == 1))\n",
    "    FN = np.sum((predicted_labels == 0) & (true_labels == 1))\n",
    "    recall_value = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    return recall_value\n",
    "\n",
    "def f1_score(predicted_labels, true_labels):\n",
    "    \"\"\"\n",
    "    F1 score is the harmonic mean of precision and recall\n",
    "    \n",
    "    Args:\n",
    "        predicted_labels (np.ndarray[int, 1]): the integer labels from the predictions. Uni-dimensional\n",
    "        true_labels (np.ndarray[int, 1]): the integer labels from the gold standard. Uni-dimensional\n",
    "    \n",
    "    Returns:\n",
    "        f1_score_value (double)\n",
    "        \n",
    "    \"\"\"\n",
    "    f1_score_value = 0.\n",
    "    P = precision(predicted_labels, true_labels)\n",
    "    R = recall(predicted_labels, true_labels)\n",
    "    f1_score_value = 2 * P * R / (P + R) if (P + R) > 0 else 0\n",
    "    return f1_score_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "cm = confusion_matrix(yTrue, yPred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred) \n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "print(\"F1 Score:\", f1)\n",
    "\n",
    "precision = precision_score(y_true, y_pred)\n",
    "print(\"Precision Score:\", precision)\n",
    "\n",
    "recall = recall_score(y_true, y_pred)\n",
    "print(\"Recall Score:\", recall)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
